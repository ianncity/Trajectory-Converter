{"file_path":"consolidated_trajectories.jsonl","file_name":"consolidated_trajectories.jsonl","file_extension":"jsonl","file_size":0,"mime_type":"application/octet-stream","encoding":null,"is_text_file":true,"line_count":1,"content":"","content_preview":"","last_modified":"2025-12-14T18:54:16.668077","created_timestamp":"2025-12-14T18:53:13.714742","access_timestamp":"2025-12-14T18:54:16.668077"}
{"file_path":"large_test_trajectory.json","file_name":"large_test_trajectory.json","file_extension":"json","file_size":4963,"mime_type":"application/json","encoding":"ascii","is_text_file":true,"line_count":44,"content":"{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"I need help understanding complex algorithms and data structures. Can you explain how to implement a balanced binary search tree?\"\n    },\n    {\n      \"role\": \"assistant\", \n      \"content\": \"I'll help you understand balanced binary search trees! A balanced BST is a self-balancing binary search tree that maintains its height logarithmic to the number of nodes, ensuring O(log n) time complexity for search, insert, and delete operations. The most common types are AVL trees and Red-Black trees. Let me walk you through the key concepts and implementation details step by step.\"\n    },\n    {\n      \"role\": \"human\",\n      \"content\": \"Start with the basic concepts. What makes a tree balanced?\"\n    },\n    {\n      \"role\": \"ai\",\n      \"content\": \"Great question! A binary tree is considered balanced when the heights of the two subtrees of any node differ by at most one. This property ensures that the tree remains approximately complete, preventing degeneration into a linked list. For AVL trees specifically, we maintain a balance factor for each node, calculated as: balance_factor = height(left_subtree) - height(right_subtree). The balance factor must always be -1, 0, or 1. If it goes beyond this range, we need to perform rotations to restore balance. The key insight is that by keeping the tree balanced, we guarantee that the maximum height is O(log n), which gives us the desired time complexity for all operations.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Now explain the rotation operations. How do they work?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Rotations are the core mechanism for maintaining balance in AVL trees. There are four types of rotations: Left Rotation, Right Rotation, Left-Right Rotation, and Right-Left Rotation. Let me explain each with code examples. A left rotation moves a node down to the left, promoting its right child. This happens when we have an unbalanced right-heavy tree. The right child becomes the new root of that subtree. Similarly, a right rotation moves a node down to the right, promoting its left child. The complexity comes when we have double imbalances that require two rotations - like Left-Right and Right-Left rotations. These are also called double rotations because they involve two single rotations in sequence. The key is to always rotate in the direction that restores the balance factor to the acceptable range of [-1, 1].\"\n    },\n    {\n      \"role\": \"human\",\n      \"content\": \"Can you show me the actual implementation code for these rotations?\"\n    },\n    {\n      \"role\": \"ai\",\n      \"content\": \"Absolutely! Here's a complete implementation of AVL tree rotations in Python. The left rotation function takes a node as input and returns the new root after rotation. We first identify the right child, then perform the rotation by making the original node the left child of the right child, and making the right child's left subtree become the original node's right subtree. The right rotation is symmetric. For double rotations, we first perform the appropriate single rotation, then the second rotation. It's crucial to update heights after each rotation and to return the new root node. The key is to always maintain the BST property while restoring balance. Here's the complete code structure: [extensive code implementation follows with detailed comments explaining each step of the rotation process, edge cases, and maintenance of tree properties]\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What about insertion and deletion operations? How do they maintain balance?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Insertion and deletion in AVL trees follow the standard BST operations but with additional balance maintenance steps. For insertion, we first perform a standard BST insert, then work our way back up the tree checking balance factors. If we find an imbalance, we perform the appropriate rotation(s). The process involves four main steps: 1) Insert the new node using BST insertion, 2) Update heights along the path from the new node to the root, 3) Check balance factors at each node, 4) Perform rotations if needed. Deletion is more complex because it can cause imbalances at multiple levels. After a standard BST deletion, we need to update heights and check balance factors all the way up to the root. If we find an imbalance, we perform rotations. This might trigger imbalances at higher levels, so we continue checking and rotating until we reach the root or find a balanced subtree. The key is that each rotation restores local balance, and we propagate this process up the tree until we reach the root. The overall time complexity remains O(log n) because the height is guaranteed to be logarithmic. Therefore, the algorithm efficiently maintains tree balance while preserving the core BST operations.\"\n    }\n  ]\n}","content_preview":"{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"I need help understanding complex algorithms and data structures. Can you explain how to implement a balanced binary search tree?\"\n    }","last_modified":"2025-12-14T18:54:09.528654","created_timestamp":"2025-12-14T18:53:58.572920","access_timestamp":"2025-12-14T18:54:31.306313"}
{"file_path":"README.md","file_name":"README.md","file_extension":"md","file_size":7197,"mime_type":"application/octet-stream","encoding":"utf-8","is_text_file":true,"line_count":226,"content":"# TrajectoryScraper Final - Complete Solution\n\n## Overview\n\nThis is a comprehensive trajectory segmentation tool that now includes integrated workspace export functionality. All features are consolidated into a single Python file for maximum portability and ease of use.\n\n## Files Created\n\n1. **`TrajectoryScraper_final.py`** - Complete solution with both trajectory segmentation and workspace export\n2. **`workspace_export.jsonl`**, **`custom_workspace.jsonl`**, **`final_export.jsonl`** - Generated JSONL exports\n3. **`README.md`** - This documentation file\n\n## Codebase Analysis\n\n### TrajectoryScraper_final.py - Complete Implementation\nThe main file now contains two integrated modes:\n\n#### 1. Trajectory Segmentation (Default Mode)\n- **Purpose**: Process conversational trajectory data from JSON files\n- **Key Features**:\n  - Extracts messages from trajectory files\n  - Creates balanced segments with optimal token utilization\n  - Targets 50-70% preservation rate for training data\n  - Uses tiktoken for accurate token counting\n  - Handles both optimized and minimal segment creation strategies\n\n#### 2. Workspace Export Mode\n- **Purpose**: Export all files in workspace to JSONL format\n- **Key Features**:\n  - Comprehensive file discovery and metadata extraction\n  - Intelligent encoding detection using chardet\n  - MIME type detection and file type classification\n  - Content extraction with error handling\n  - JSON Lines format for easy processing\n\n### Usage Instructions\n\n#### Trajectory Segmentation (Default)\n```bash\npython TrajectoryScraper_final.py\n```\nThis processes all JSON/JSONL files in the current directory and creates segmented outputs.\n\n#### Workspace Export\n```bash\npython TrajectoryScraper_final.py export\n```\nThis exports all workspace files to `workspace_export.jsonl`.\n\n#### Custom Export File\n```bash\npython TrajectoryScraper_final.py export custom_output.jsonl\n```\n\n#### Help\n```bash\npython TrajectoryScraper_final.py help\n```\n\n## JSONL Export Format\n\nEach line in the output JSONL file contains a JSON object with comprehensive file information:\n\n```json\n{\n  \"file_path\": \"relative/path/to/file.py\",\n  \"file_name\": \"filename.py\",\n  \"file_extension\": \"py\",\n  \"file_size\": 12345,\n  \"mime_type\": \"text/x-python\",\n  \"encoding\": \"utf-8\",\n  \"is_text_file\": true,\n  \"line_count\": 311,\n  \"content\": \"完整的文件内容...\",\n  \"content_preview\": \"文件内容的前200个字符...\",\n  \"last_modified\": \"2025-12-15T00:10:36.859Z\",\n  \"created_timestamp\": \"2025-12-14T18:07:37.461207\",\n  \"access_timestamp\": \"2025-12-14T18:07:58.623142\"\n}\n```\n\n## Integration Benefits\n\n### Unified Solution\n- **Single File**: All functionality in one portable Python script\n- **Command Line Interface**: Easy to use with clear modes\n- **Backward Compatible**: Original trajectory processing unchanged\n- **Extended Functionality**: New export capabilities added seamlessly\n\n### Enhanced Features\n- **Multi-Mode Operation**: Toggle between segmentation and export modes\n- **Comprehensive Error Handling**: Robust error handling for both modes\n- **Progress Reporting**: Detailed progress and summary information\n- **Flexible Output**: Customizable output file names\n\n### Dependencies\nAll required dependencies are included in the single file:\n- `json` - JSON parsing and generation\n- `tiktoken` - Token counting for trajectory analysis\n- `numpy` - Numerical operations for statistics\n- `pathlib` - Cross-platform path handling\n- `datetime` - Timestamp generation\n- `mimetypes` - MIME type detection\n- `chardet` - Encoding detection\n- `typing` - Type hints\n\n## Current Workspace Results\n\n```\nExport completed!\nTotal files: 4\nSuccessful: 4\nFailed: 0\nTotal size: 200,809 bytes\nText files: 4\nBinary files: 0\nOutput file: final_export.jsonl\n```\n\n### Files in Workspace\n1. **TrajectoryScraper_final.py** (Complete solution - main script)\n2. **README.md** (Documentation)\n3. **workspace_export.jsonl** (Generated export)\n4. **custom_workspace.jsonl** (Custom export)\n5. **final_export.jsonl** (Final export)\n\n## Technical Architecture\n\n### Modular Design\nThe solution uses a modular approach within a single file:\n\n```python\n# Core trajectory processing functions\ndef count_tokens()\ndef extract_messages()\ndef create_balanced_segments()\ndef process_trajectory_file()\n\n# Workspace export functions\ndef detect_encoding()\ndef get_mime_type()\ndef read_file_content()\ndef get_file_info()\ndef export_workspace()\n\n# Main orchestration\ndef main()\n```\n\n### Command Line Interface\nThe main function provides a clean CLI interface:\n\n- **No arguments**: Run trajectory segmentation\n- **export**: Export workspace to JSONL\n- **export [filename]**: Export to custom filename\n- **help**: Show usage information\n\n## Use Cases\n\n### 1. Trajectory Analysis\n- Process conversational datasets\n- Create training segments for LLMs\n- Optimize token utilization\n\n### 2. Workspace Management\n- Complete workspace backup\n- File analysis and inspection\n- Code quality assessment\n\n### 3. Machine Learning\n- Training data preparation\n- Dataset creation for fine-tuning\n- Content analysis and preprocessing\n\n### 4. Documentation\n- Automatic code documentation\n- Content indexing\n- Search engine preparation\n\n## Performance Characteristics\n\n### Memory Efficient\n- Processes files individually\n- Streams output directly to JSONL\n- No memory accumulation issues\n\n### Robust Error Handling\n- Graceful handling of encoding issues\n- Continues processing on individual file failures\n- Comprehensive error reporting\n\n### Cross-Platform Compatibility\n- Windows: Full support with proper path handling\n- Linux/macOS: Cross-platform compatibility\n- Python 3.6+: Modern Python features\n\n## Future Enhancements\n\nPotential improvements could include:\n- Configuration file support for export options\n- Filter patterns for selective file inclusion\n- Compression options for large workspaces\n- Parallel processing for better performance\n- Progress bars for large operations\n- Incremental export capabilities\n- Integration with version control systems\n\n## Troubleshooting\n\n### Common Issues\n1. **Encoding Errors**: Automatic detection and fallback handling\n2. **Large Files**: No size limits, optimized for large datasets\n3. **Permission Errors**: Ensure read permissions for all files\n4. **Missing Dependencies**: All dependencies are included in the file\n\n### Debug Information\nThe script provides detailed progress information:\n- File processing status\n- Success/failure counts\n- Total size calculations\n- File type distributions\n- Export statistics\n\n## Conclusion\n\nThis integrated solution provides a robust, comprehensive tool for both trajectory segmentation and workspace export. The consolidation into a single file makes it highly portable and easy to deploy, while maintaining all the sophisticated features of both original tools. The solution is production-ready with proper error handling, cross-platform compatibility, and extensive metadata extraction suitable for various analysis, backup, and processing needs.","content_preview":"# TrajectoryScraper Final - Complete Solution\n\n## Overview\n\nThis is a comprehensive trajectory segmentation tool that now includes integrated workspace export functionality. All features are consolida","last_modified":"2025-12-14T18:10:57.077383","created_timestamp":"2025-12-14T18:08:23.926946","access_timestamp":"2025-12-14T18:42:15.967766"}
{"file_path":"test_trajectory.json","file_name":"test_trajectory.json","file_extension":"json","file_size":2310,"mime_type":"application/json","encoding":"utf-8","is_text_file":true,"line_count":28,"content":"{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the capital of France? Please provide a detailed explanation.\"\n    },\n    {\n      \"role\": \"assistant\", \n      \"content\": \"The capital of France is Paris. It's a beautiful city known for its art, culture, and cuisine. Paris has been the political and cultural center of France for centuries. The city is home to famous landmarks like the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris is also known for its high fashion, gourmet cuisine, and rich literary history. Many famous writers, artists, and philosophers have lived and worked in Paris throughout history.\"\n    },\n    {\n      \"role\": \"human\",\n      \"content\": \"Tell me more about Paris and its history.\"\n    },\n    {\n      \"role\": \"ai\",\n      \"content\": \"Paris has a fascinating history that spans over 2,000 years. Originally founded by the Parisii tribe around the 3rd century BC, the city was later conquered by the Romans who called it Lutetia. During the Middle Ages, Paris became an important center of learning and commerce. The city flourished during the Renaissance and later became the capital of the French kingdom. The French Revolution in 1789 dramatically transformed Paris and France as a whole. In the 19th century, Baron Haussmann renovated the city, creating the wide boulevards and elegant architecture we see today. Paris has continued to be a global center for art, culture, fashion, and intellectual discourse.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"What are the main attractions in Paris?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Paris has numerous world-famous attractions. The Eiffel Tower is perhaps the most iconic, offering stunning views of the city from its observation decks. The Louvre Museum houses the famous Mona Lisa and countless other masterpieces. Notre-Dame Cathedral is a masterpiece of French Gothic architecture. Other notable attractions include the Arc de Triomphe, Sacré-Cœur Basilica, the Musée d'Orsay, and the Palace of Versailles nearby. The Champs-Élysées is a famous shopping boulevard, while Montmartre offers charming streets and artistic heritage. Paris also boasts beautiful parks like the Luxembourg Gardens and the Bois de Boulogne.\"\n    }\n  ]\n}","content_preview":"{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the capital of France? Please provide a detailed explanation.\"\n    },\n    {\n      \"role\": \"assistant\", \n      \"content\": \"The ca","last_modified":"2025-12-14T18:53:44.749516","created_timestamp":"2025-12-14T18:53:40.104890","access_timestamp":"2025-12-14T18:54:31.306313"}
{"file_path":"TrajectoryScraper_final.py","file_name":"TrajectoryScraper_final.py","file_extension":"py","file_size":40157,"mime_type":"text/x-python","encoding":"ascii","is_text_file":true,"line_count":930,"content":"import json\nimport tiktoken\nimport numpy as np\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport math\nimport mimetypes\nfrom datetime import datetime\nimport chardet\n\n# Initialize tokenizer\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(str(text)))\n\ndef normalize_role(role: str) -> str:\n    \"\"\"Normalize role names to standard format: system, user, assistant\"\"\"\n    if not role:\n        return \"user\"\n    \n    role_lower = role.lower().strip()\n    \n    # Map various role representations to standard format\n    role_mappings = {\n        # System variations\n        \"system\": \"system\",\n        \"sys\": \"system\", \n        \"s\": \"system\",\n        \"context\": \"system\",\n        \"instruction\": \"system\",\n        \n        # User variations  \n        \"user\": \"user\",\n        \"human\": \"user\",\n        \"human_user\": \"user\",\n        \"u\": \"user\",\n        \"customer\": \"user\",\n        \"client\": \"user\",\n        \"question\": \"user\",\n        \"query\": \"user\",\n        \n        # Assistant variations\n        \"assistant\": \"assistant\",\n        \"ai\": \"assistant\",\n        \"bot\": \"assistant\", \n        \"a\": \"assistant\",\n        \"agent\": \"assistant\",\n        \"model\": \"assistant\",\n        \"response\": \"assistant\",\n        \"answer\": \"assistant\",\n        \"completion\": \"assistant\"\n    }\n    \n    # Direct mapping\n    if role_lower in role_mappings:\n        return role_mappings[role_lower]\n    \n    # Fuzzy matching for partial matches\n    if \"system\" in role_lower or \"context\" in role_lower or \"instruction\" in role_lower:\n        return \"system\"\n    elif \"user\" in role_lower or \"human\" in role_lower or \"customer\" in role_lower:\n        return \"user\"  \n    elif \"assistant\" in role_lower or \"ai\" in role_lower or \"bot\" in role_lower or \"agent\" in role_lower:\n        return \"assistant\"\n    \n    # Default fallback\n    return \"user\"\n\ndef extract_messages(data: Any) -> List[Dict[str, str]]:\n    \"\"\"Extract messages from the file.\"\"\"\n    if isinstance(data, dict) and 'messages' in data:\n        messages_field = data['messages']\n        \n        # Handle case where messages is a JSON string that needs to be parsed\n        if isinstance(messages_field, str):\n            try:\n                messages_array = json.loads(messages_field)\n                if isinstance(messages_array, list):\n                    normalized_messages = []\n                    for msg in messages_array:\n                        if isinstance(msg, dict) and 'role' in msg and 'content' in msg:\n                            # Normalize the role and ensure content exists\n                            normalized_msg = {\n                                'role': normalize_role(msg['role']),\n                                'content': msg.get('content', '')\n                            }\n                            normalized_messages.append(normalized_msg)\n                    return normalized_messages\n            except json.JSONDecodeError:\n                pass\n        elif isinstance(messages_field, list):\n            normalized_messages = []\n            for msg in messages_field:\n                if isinstance(msg, dict) and 'role' in msg and 'content' in msg:\n                    # Normalize the role and ensure content exists\n                    normalized_msg = {\n                        'role': normalize_role(msg['role']),\n                        'content': msg.get('content', '')\n                    }\n                    normalized_messages.append(normalized_msg)\n            return normalized_messages\n    \n    return []\n\ndef load_trajectory_file(file_path: Path) -> tuple[List[Dict[str, str]], str]:\n    \"\"\"\n    Load trajectory file with dual JSON/JSONL support for LLM training data.\n    \n    Automatically detects and processes both JSON and JSONL formats:\n    - JSON: Single object or array with messages field\n    - JSONL: One JSON object per line (supports multiple trajectories)\n    \n    Returns:\n        Tuple of (messages_list, file_format_detected)\n    \"\"\"\n    all_messages = []\n    file_format = None\n    \n    try:\n        # Detect file format and process accordingly\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                # Read first few lines to detect format\n                first_lines = []\n                for i, line in enumerate(f):\n                    if i < 5:  # Check first 5 lines\n                        first_lines.append(line.strip())\n                    else:\n                        break\n            \n            # JSONL detection: multiple lines, each is a complete JSON object\n            jsonl_indicators = []\n            for line in first_lines:\n                if line and line.startswith('{') and line.endswith('}'):\n                    try:\n                        json.loads(line)\n                        jsonl_indicators.append(True)\n                    except json.JSONDecodeError:\n                        jsonl_indicators.append(False)\n            \n            # If most lines are valid JSON objects, treat as JSONL\n            if len(jsonl_indicators) > 0 and sum(jsonl_indicators) >= len(jsonl_indicators) * 0.6:\n                file_format = \"JSONL\"\n                return load_jsonl_format(file_path, all_messages), file_format\n            else:\n                file_format = \"JSON\"\n                return load_json_format(file_path, all_messages), file_format\n                \n        except Exception as e:\n            # Fallback: try JSON format first, then JSONL\n            try:\n                file_format = \"JSON (fallback)\"\n                return load_json_format(file_path, all_messages), file_format\n            except Exception:\n                file_format = \"JSONL (fallback)\"\n                return load_jsonl_format(file_path, all_messages), file_format\n    \n    except Exception as e:\n        raise Exception(f\"Failed to load trajectory file ({file_format}): {str(e)}\")\n\ndef load_json_format(file_path: Path, all_messages: List[Dict[str, str]]) -> List[Dict[str, str]]:\n    \"\"\"Load single JSON file (object or array).\"\"\"\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n        messages = extract_messages(data)\n        all_messages.extend(messages)\n    return all_messages\n\ndef load_jsonl_format(file_path: Path, all_messages: List[Dict[str, str]]) -> List[Dict[str, str]]:\n    \"\"\"Load JSONL file (one JSON object per line).\"\"\"\n    line_count = 0\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line_num, line in enumerate(f, 1):\n            line = line.strip()\n            line_count += 1\n            if line:  # Skip empty lines\n                try:\n                    data = json.loads(line)\n                    messages = extract_messages(data)\n                    if messages:\n                        all_messages.extend(messages)\n                except json.JSONDecodeError as e:\n                    print(f\"    Warning: JSON decode error on line {line_num}: {e}\")\n                    continue\n    \n    print(f\"    Processed {line_count} lines from JSONL file\")\n    return all_messages\n\ndef is_substantial_content(content: str) -> bool:\n    \"\"\"Check if content is substantial enough to include (filters boilerplate).\"\"\"\n    if not content or len(content.strip()) < 50:\n        return False\n    \n    # Skip obvious boilerplate\n    boilerplate_phrases = [\n        \"thank you\", \"thanks\", \"you're welcome\", \"my pleasure\",\n        \"glad to help\", \"hope this helps\", \"let me know if\",\n        \"is there anything else\", \"feel free to ask\",\n        \"good question\", \"excellent question\", \"that's a great question\"\n    ]\n    \n    content_lower = content.lower()\n    boilerplate_count = sum(1 for phrase in boilerplate_phrases if phrase in content_lower)\n    \n    # If more than 30% of content is boilerplate, skip it\n    if boilerplate_count > 0 and len(content) < 200:\n        return False\n    \n    return True\n\ndef find_natural_stop_point(content: str, target_tokens: int) -> int:\n    \"\"\"\n    Find a natural stopping point in content after reaching target tokens.\n    Looks for sentence endings, paragraph breaks, or logical conclusion points.\n    \"\"\"\n    if len(content) <= target_tokens * 4:  # Rough token to character ratio\n        return len(content)\n    \n    # Check for natural stopping points in the last portion of content\n    search_start = max(0, len(content) - target_tokens * 6)  # Search last ~6000 chars\n    \n    # Priority order for natural stopping points\n    stop_patterns = [\n        # Conclusion indicators\n        r'[.!?]\\s*$',  # End of sentence\n        r'\\n\\s*$',     # End of paragraph\n        r'Therefore[,.\\s]',\n        r'In conclusion[,.\\s]',\n        r'To summarize[,.\\s]',\n        r'In summary[,.\\s]',\n        r'Overall[,.\\s]',\n        \n        # Code block endings\n        r'```\\s*$',\n        r'\\}\\s*$',     # End of code object\n        r'\\]\\s*$',     # End of array/object\n        \n        # Question transitions\n        r'\\?\\s*$',     # End of question\n        r'\\n\\s*Q:',    # New question\n        r'\\n\\s*Next:',\n        \n        # Step completions\n        r'Step \\d+[:.\\s]',\n        r'Finally[,.\\s]',\n        r'Last[,.\\s]',\n        \n        # Natural breaks\n        r'\\n\\s*\\n',    # Double newline\n        r'[:.!?]\\s+\\n', # Sentence end followed by newline\n    ]\n    \n    # Search for the best stopping point\n    for pattern in stop_patterns:\n        import re\n        matches = list(re.finditer(pattern, content[search_start:], re.IGNORECASE))\n        if matches:\n            # Return the position of the first good match\n            match = matches[0]\n            return search_start + match.end()\n    \n    # If no good pattern found, cut at a sentence boundary near target\n    sentences = re.split(r'[.!?]+\\s+', content[search_start:])\n    if len(sentences) > 1:\n        # Find the sentence that gets us closest to target without exceeding it\n        cumulative_length = search_start\n        for sentence in sentences:\n            sentence_end = cumulative_length + len(sentence) + 2  # +2 for punctuation and space\n            if sentence_end <= target_tokens * 4.5:  # Allow some buffer\n                cumulative_length = sentence_end\n            else:\n                break\n        return cumulative_length\n    \n    # Fallback: cut at word boundary near target\n    words = content[search_start:].split()\n    cumulative_length = search_start\n    for word in words:\n        if cumulative_length + len(word) + 1 <= target_tokens * 4.5:\n            cumulative_length += len(word) + 1\n        else:\n            break\n    return cumulative_length\n\ndef create_focused_chain_of_thought_segments(messages: List[Dict[str, str]],\n                                           target_tokens: int = 6144,\n                                           target_preservation: float = 70.0) -> List[Dict[str, Any]]:\n    \"\"\"\n    Create focused segments optimized for chain of thought learning.\n    Uses intelligent stopping: looks for natural breaks after 5000 tokens, max 6144.\n    \"\"\"\n    if not messages:\n        return []\n    \n    segments = []\n    assistant_indices = [i for i, msg in enumerate(messages) if msg.get('role') == 'assistant']\n    \n    if not assistant_indices:\n        return []\n    \n    print(f\"  Found {len(assistant_indices)} assistant messages\")\n    \n    # Calculate target total tokens based on preservation rate\n    original_messages_text = \"\\n\".join([f\"{m.get('role', 'unknown')}: {m.get('content', '')}\" for m in messages])\n    original_tokens = count_tokens(original_messages_text)\n    target_total_tokens = int(original_tokens * (target_preservation / 100.0))\n    \n    print(f\"  Original tokens: {original_tokens}\")\n    print(f\"  Target preservation: {target_preservation}%\")\n    print(f\"  Target total tokens: {target_total_tokens}\")\n    print(f\"  Max tokens per segment: {target_tokens}\")\n    print(f\"  Smart stopping: Look for natural breaks after 5000 tokens\")\n    \n    # Strategy: Create focused segments with intelligent stopping\n    used_indices = set()\n    total_generated_tokens = 0\n    tokens_remaining = target_total_tokens\n    \n    # Filter for substantial assistant messages\n    substantial_assistants = []\n    for idx in assistant_indices:\n        msg = messages[idx]\n        if is_substantial_content(msg.get('content', '')):\n            substantial_assistants.append(idx)\n    \n    print(f\"  Found {len(substantial_assistants)} substantial assistant responses\")\n    \n    i = 0\n    while i < len(substantial_assistants) and tokens_remaining > 1000:\n        assistant_idx = substantial_assistants[i]\n        \n        if assistant_idx in used_indices:\n            i += 1\n            continue\n        \n        # Build segment progressively with smart stopping\n        best_segment = None\n        best_tokens = 0\n        best_start = max(0, assistant_idx - 3)\n        best_end = assistant_idx + 1\n        \n        # Try building segments with different context sizes\n        for context_before in range(1, min(8, assistant_idx + 1)):  # More context options\n            for context_after in range(1, 5):  # More future context options\n                start_idx = max(0, assistant_idx - context_before)\n                end_idx = min(len(messages), assistant_idx + context_after + 1)\n                \n                # Skip if any indices are already used\n                range_indices = set(range(start_idx, end_idx))\n                if used_indices.intersection(range_indices):\n                    continue\n                \n                segment_msgs = messages[start_idx:end_idx]\n                \n                # Ensure we have context and the target assistant\n                context_msgs = [msg for msg in segment_msgs if msg.get('role') != 'assistant']\n                assistant_msgs = [msg for msg in segment_msgs if msg.get('role') == 'assistant']\n                \n                if not context_msgs or assistant_idx not in [messages.index(msg) for msg in assistant_msgs]:\n                    continue\n                \n                segment_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in segment_msgs])\n                segment_tokens = count_tokens(segment_text)\n                \n                # Smart stopping logic: prefer segments near 5000 tokens with natural breaks\n                if segment_tokens >= 800 and segment_tokens <= target_tokens:\n                    # Check if this would be a good stopping point\n                    is_smart_stop = segment_tokens >= 5000\n                    \n                    # Score segments: prefer smart stops, then larger segments, then efficiency\n                    score = 0\n                    if is_smart_stop:\n                        score += 1000  # Heavy bonus for smart stopping points\n                    score += segment_tokens / 10  # Prefer larger segments\n                    score += (min(target_tokens, tokens_remaining) - segment_tokens) / 100  # Efficiency bonus\n                    \n                    if score > best_tokens:  # Using best_tokens as score storage\n                        best_segment = segment_msgs\n                        best_tokens = score  # Store score, not token count\n                        best_start = start_idx\n                        best_end = end_idx\n                        best_token_count = segment_tokens  # Store actual token count\n        \n        # Create the best segment found\n        if best_segment and best_tokens >= 800:\n            target_msg = messages[assistant_idx]\n            context = [msg for msg in best_segment if msg.get('role') != 'assistant']\n            \n            segments.append({\n                'input_context': context,\n                'target_action': target_msg['content'],\n                'metadata': {\n                    'segment_tokens': best_token_count,\n                    'start_idx': best_start,\n                    'end_idx': best_end,\n                    'assistant_index': assistant_idx,\n                    'segment_type': 'smart_stop_coT' if best_token_count >= 5000 else 'focused_coT',\n                    'context_length': len(context),\n                    'target_length': len(target_msg['content']) if target_msg['content'] else 0,\n                    'num_assistants_in_segment': 1,\n                    'quality_filter': 'substantial_content',\n                    'role_normalization': True,\n                    'smart_stopping': best_token_count >= 5000\n                }\n            })\n            \n            total_generated_tokens += best_token_count\n            tokens_remaining -= best_token_count\n            used_indices.update(range(best_start, best_end))\n            \n            stop_type = \"smart stop\" if best_token_count >= 5000 else \"focused\"\n            print(f\"    Created {stop_type} segment: {best_token_count} tokens, {len(context)} context messages\")\n            i += 1\n        else:\n            # Try with minimal context if no good segment found\n            context_start = max(0, assistant_idx - 2)\n            context_end = min(len(messages), assistant_idx + 2)\n            \n            context_msgs = messages[context_start:assistant_idx]\n            segment_msgs = messages[context_start:context_end]\n            \n            if (len(context_msgs) > 0 and\n                not used_indices.intersection(range(context_start, context_end)) and\n                is_substantial_content(target_msg.get('content', ''))):\n                \n                segment_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in segment_msgs])\n                segment_tokens = count_tokens(segment_text)\n                \n                if segment_tokens >= 500 and segment_tokens <= min(target_tokens, tokens_remaining):\n                    segments.append({\n                        'input_context': context_msgs,\n                        'target_action': target_msg['content'],\n                        'metadata': {\n                            'segment_tokens': segment_tokens,\n                            'start_idx': context_start,\n                            'end_idx': context_end,\n                            'assistant_index': assistant_idx,\n                            'segment_type': 'minimal_focused',\n                            'context_length': len(context_msgs),\n                            'target_length': len(target_msg['content']) if target_msg['content'] else 0,\n                            'num_assistants_in_segment': 1,\n                            'quality_filter': 'minimal_substantial',\n                            'role_normalization': True,\n                            'smart_stopping': False\n                        }\n                    })\n                    \n                    total_generated_tokens += segment_tokens\n                    tokens_remaining -= segment_tokens\n                    used_indices.update(range(context_start, context_end))\n                    \n                    print(f\"    Created minimal focused segment: {segment_tokens} tokens\")\n                    i += 1\n                else:\n                    i += 1\n            else:\n                i += 1\n    \n    # Sort segments by start_idx\n    segments.sort(key=lambda x: x['metadata']['start_idx'])\n    \n    print(f\"  Total generated tokens: {total_generated_tokens}\")\n    print(f\"  Actual preservation: {(total_generated_tokens / original_tokens * 100):.1f}%\")\n    print(f\"  Number of focused CoT segments: {len(segments)}\")\n    \n    return segments\n\ndef calculate_preservation_rate(segments: List[Dict[str, Any]], original_tokens: int) -> float:\n    \"\"\"Calculate the actual preservation rate.\"\"\"\n    total_tokens = 0\n    for segment in segments:\n        total_tokens += segment['metadata']['segment_tokens']\n    \n    return (total_tokens / original_tokens * 100) if original_tokens > 0 else 0\n\ndef process_trajectory_file(file_path: Path, output_dir: Path) -> Dict[str, Any]:\n    \"\"\"\n    Process a single trajectory file with dual JSON/JSONL support.\n    \n    Automatically detects JSON vs JSONL format and applies role normalization\n    for system/user/assistant conversion.\n    \"\"\"\n    result = {\n        'file': file_path.name,\n        'success': False,\n        'error': None,\n        'segments': 0,\n        'preservation': 0.0,\n        'file_format': None\n    }\n    \n    try:\n        # Load file with dual JSON/JSONL support and format detection\n        messages, file_format = load_trajectory_file(file_path)\n        \n        if not messages:\n            result['error'] = \"No messages found\"\n            return result\n        \n        print(f\"  Detected format: {file_format}\")\n        print(f\"  Messages: {len(messages)}, Assistant: {sum(1 for m in messages if m.get('role') == 'assistant')}\")\n        \n        # Show role distribution for verification (auto-conversion to system/user/assistant)\n        role_counts = {}\n        for msg in messages:\n            role = msg.get('role', 'unknown')\n            role_counts[role] = role_counts.get(role, 0) + 1\n        print(f\"  Role distribution: {role_counts}\")\n        print(f\"  Auto-role conversion: system/user/assistant format applied\")\n        \n        # Create focused segments optimized for chain of thought learning\n        segments = create_focused_chain_of_thought_segments(messages)\n        \n        if not segments:\n            result['error'] = \"Could not create segments\"\n            return result\n        \n        # Save to file in JSONL format for LLM training\n        output_file = output_dir / f\"{file_path.stem}_final.jsonl\"\n        with open(output_file, 'w', encoding='utf-8') as f:\n            for segment in segments:\n                f.write(json.dumps(segment, ensure_ascii=False) + '\\n')\n        \n        # Calculate preservation rate\n        original_messages_text = \"\\n\".join([f\"{m.get('role', 'unknown')}: {m.get('content', '')}\" for m in messages])\n        original_tokens = count_tokens(original_messages_text)\n        preservation = calculate_preservation_rate(segments, original_tokens)\n        \n        result['success'] = True\n        result['segments'] = len(segments)\n        result['preservation'] = preservation\n        result['output_file'] = str(output_file)\n        result['original_tokens'] = original_tokens\n        result['file_format'] = file_format\n        \n        print(f\"  Created {len(segments)} final focused CoT segments, {preservation:.1f}% preservation\")\n        print(f\"  Role normalization: Applied to all messages\")\n        print(f\"  Output format: JSONL for LLM training\")\n        \n        # Show segment types and token stats\n        segment_types = {}\n        token_counts = []\n        context_lengths = []\n        target_lengths = []\n        assistant_counts = []\n        \n        for s in segments:\n            seg_type = s['metadata']['segment_type']\n            segment_types[seg_type] = segment_types.get(seg_type, 0) + 1\n            token_counts.append(s['metadata']['segment_tokens'])\n            context_lengths.append(s['metadata']['context_length'])\n            target_lengths.append(s['metadata']['target_length'])\n            assistant_counts.append(s['metadata'].get('num_assistants_in_segment', 1))\n        \n        print(f\"  Segment types: {segment_types}\")\n        if token_counts:\n            print(f\"  Token stats - Min: {min(token_counts)}, Max: {max(token_counts)}, Avg: {np.mean(token_counts):.0f}\")\n            print(f\"  Token utilization - Min: {min(token_counts)/6144*100:.1f}%, Max: {max(token_counts)/6144*100:.1f}%, Avg: {np.mean(token_counts)/6144*100:.1f}%\")\n            print(f\"  Context length - Avg: {np.mean(context_lengths):.1f} messages\")\n            print(f\"  Target length - Avg: {np.mean(target_lengths):.0f} characters\")\n            print(f\"  Assistants per segment - Avg: {np.mean(assistant_counts):.1f}\")\n        \n    except Exception as e:\n        result['error'] = str(e)\n    \n    return result\n\n# =============================================================================\n# WORKSPACE EXPORT FUNCTIONALITY\n# =============================================================================\n\ndef detect_encoding(file_path: Path) -> str:\n    \"\"\"Detect file encoding using chardet.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            raw_data = f.read(10000)  # Read first 10KB for detection\n            result = chardet.detect(raw_data)\n            return result.get('encoding', 'utf-8')\n    except Exception:\n        return 'utf-8'  # Fallback to utf-8\n\ndef get_mime_type(file_path: Path) -> str:\n    \"\"\"Get MIME type for a file.\"\"\"\n    mime_type, _ = mimetypes.guess_type(str(file_path))\n    return mime_type or 'application/octet-stream'\n\ndef read_file_content(file_path: Path, encoding: str) -> tuple[str, bool, int]:\n    \"\"\"Read file content and return content, is_text, and line count.\"\"\"\n    try:\n        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n            content = f.read()\n            lines = content.split('\\n')\n            return content, True, len(lines)\n    except Exception:\n        # If text reading fails, try binary reading\n        try:\n            with open(file_path, 'rb') as f:\n                content = f.read()\n                # Try to decode as UTF-8, replace errors\n                content_str = content.decode('utf-8', errors='replace')\n                lines = content_str.split('\\n')\n                return content_str, False, len(lines)\n        except Exception as e:\n            return f\"[Error reading file: {str(e)}]\", False, 0\n\ndef get_file_info(file_path: Path) -> Dict[str, Any]:\n    \"\"\"Get comprehensive information about a file.\"\"\"\n    try:\n        stat = file_path.stat()\n        \n        # Detect encoding and MIME type\n        encoding = detect_encoding(file_path)\n        mime_type = get_mime_type(file_path)\n        \n        # Read content\n        content, is_text, line_count = read_file_content(file_path, encoding)\n        \n        # Generate preview\n        preview = content[:200] if isinstance(content, str) else str(content)[:200]\n        \n        return {\n            \"file_path\": str(file_path.relative_to(Path.cwd())),\n            \"file_name\": file_path.name,\n            \"file_extension\": file_path.suffix[1:] if file_path.suffix else \"\",\n            \"file_size\": stat.st_size,\n            \"mime_type\": mime_type,\n            \"encoding\": encoding,\n            \"is_text_file\": is_text,\n            \"line_count\": line_count,\n            \"content\": content,\n            \"content_preview\": preview,\n            \"last_modified\": datetime.fromtimestamp(stat.st_mtime).isoformat(),\n            \"created_timestamp\": datetime.fromtimestamp(stat.st_ctime).isoformat(),\n            \"access_timestamp\": datetime.fromtimestamp(stat.st_atime).isoformat()\n        }\n    except Exception as e:\n        return {\n            \"file_path\": str(file_path.relative_to(Path.cwd())),\n            \"file_name\": file_path.name,\n            \"error\": f\"Failed to process file: {str(e)}\"\n        }\n\ndef export_workspace(output_file: str = \"workspace_export.jsonl\") -> Dict[str, Any]:\n    \"\"\"Export all files in the workspace to a JSONL file.\"\"\"\n    \n    print(f\"Starting workspace export...\")\n    print(f\"Current directory: {Path.cwd()}\")\n    \n    # Find all files in the workspace\n    workspace_path = Path.cwd()\n    all_files = []\n    \n    # Walk through all directories and files\n    for file_path in workspace_path.rglob('*'):\n        if file_path.is_file() and not any(part.startswith('.') for part in file_path.parts):\n            all_files.append(file_path)\n    \n    print(f\"Found {len(all_files)} files to export\")\n    \n    # Export to JSONL\n    results = []\n    success_count = 0\n    error_count = 0\n    \n    with open(output_file, 'w', encoding='utf-8') as f:\n        for i, file_path in enumerate(all_files, 1):\n            print(f\"[{i}/{len(all_files)}] Processing: {file_path.name}\")\n            \n            file_info = get_file_info(file_path)\n            \n            # Write to JSONL\n            json_line = json.dumps(file_info, ensure_ascii=False, separators=(',', ':'))\n            f.write(json_line + '\\n')\n            \n            results.append(file_info)\n            \n            if 'error' not in file_info:\n                success_count += 1\n            else:\n                error_count += 1\n    \n    summary = {\n        \"export_timestamp\": datetime.now().isoformat(),\n        \"total_files\": len(all_files),\n        \"successful\": success_count,\n        \"failed\": error_count,\n        \"output_file\": output_file,\n        \"workspace_path\": str(workspace_path),\n        \"file_summary\": {\n            \"total_size_bytes\": sum(r.get('file_size', 0) for r in results if 'file_size' in r),\n            \"text_files\": sum(1 for r in results if r.get('is_text_file', False)),\n            \"binary_files\": sum(1 for r in results if not r.get('is_text_file', True)),\n            \"extensions\": list(set(r.get('file_extension', '') for r in results if r.get('file_extension')))\n        }\n    }\n    \n    print(f\"\\nExport completed!\")\n    print(f\"Total files: {summary['total_files']}\")\n    print(f\"Successful: {summary['successful']}\")\n    print(f\"Failed: {summary['failed']}\")\n    print(f\"Total size: {summary['file_summary']['total_size_bytes']:,} bytes\")\n    print(f\"Text files: {summary['file_summary']['text_files']}\")\n    print(f\"Binary files: {summary['file_summary']['binary_files']}\")\n    print(f\"Output file: {output_file}\")\n    \n    return summary\n\ndef process_trajectory_file_to_single_file(file_path: Path, output_file) -> Dict[str, Any]:\n    \"\"\"Process a trajectory file and write segments directly to single output file.\"\"\"\n    result = {\n        'file': file_path.name,\n        'success': False,\n        'error': None,\n        'segments': 0,\n        'preservation': 0.0,\n        'file_format': None\n    }\n    \n    try:\n        # Load file with dual JSON/JSONL support and format detection\n        messages, file_format = load_trajectory_file(file_path)\n        \n        if not messages:\n            result['error'] = \"No messages found\"\n            return result\n        \n        print(f\"  Detected format: {file_format}\")\n        print(f\"  Messages: {len(messages)}, Assistant: {sum(1 for m in messages if m.get('role') == 'assistant')}\")\n        \n        # Show role distribution for verification (auto-conversion to system/user/assistant)\n        role_counts = {}\n        for msg in messages:\n            role = msg.get('role', 'unknown')\n            role_counts[role] = role_counts.get(role, 0) + 1\n        print(f\"  Role distribution: {role_counts}\")\n        print(f\"  Auto-role conversion: system/user/assistant format applied\")\n        \n        # Create focused segments optimized for chain of thought learning\n        segments = create_focused_chain_of_thought_segments(messages)\n        \n        if not segments:\n            result['error'] = \"Could not create segments\"\n            return result\n        \n        # Write segments directly to single output file\n        segments_written = 0\n        for segment in segments:\n            json_line = json.dumps(segment, ensure_ascii=False) + '\\n'\n            output_file.write(json_line)\n            segments_written += 1\n        \n        # Calculate preservation rate\n        original_messages_text = \"\\n\".join([f\"{m.get('role', 'unknown')}: {m.get('content', '')}\" for m in messages])\n        original_tokens = count_tokens(original_messages_text)\n        preservation = calculate_preservation_rate(segments, original_tokens)\n        \n        result['success'] = True\n        result['segments'] = segments_written\n        result['preservation'] = preservation\n        result['original_tokens'] = original_tokens\n        result['file_format'] = file_format\n        \n        print(f\"  Created {segments_written} segments, {preservation:.1f}% preservation\")\n        print(f\"  Role normalization: Applied to all messages\")\n        print(f\"  Smart stopping: Active (natural breaks after 5000 tokens)\")\n        \n        # Show segment types and token stats\n        segment_types = {}\n        token_counts = []\n        context_lengths = []\n        target_lengths = []\n        assistant_counts = []\n        smart_stops = 0\n        \n        for s in segments:\n            seg_type = s['metadata']['segment_type']\n            segment_types[seg_type] = segment_types.get(seg_type, 0) + 1\n            token_counts.append(s['metadata']['segment_tokens'])\n            context_lengths.append(s['metadata']['context_length'])\n            target_lengths.append(s['metadata']['target_length'])\n            assistant_counts.append(s['metadata'].get('num_assistants_in_segment', 1))\n            if s['metadata'].get('smart_stopping', False):\n                smart_stops += 1\n        \n        print(f\"  Segment types: {segment_types}\")\n        print(f\"  Smart stopping segments: {smart_stops}/{segments_written}\")\n        if token_counts:\n            print(f\"  Token stats - Min: {min(token_counts)}, Max: {max(token_counts)}, Avg: {np.mean(token_counts):.0f}\")\n            print(f\"  Token utilization - Min: {min(token_counts)/6144*100:.1f}%, Max: {max(token_counts)/6144*100:.1f}%, Avg: {np.mean(token_counts)/6144*100:.1f}%\")\n            print(f\"  Context length - Avg: {np.mean(context_lengths):.1f} messages\")\n            print(f\"  Target length - Avg: {np.mean(target_lengths):.0f} characters\")\n            print(f\"  Assistants per segment - Avg: {np.mean(assistant_counts):.1f}\")\n        \n    except Exception as e:\n        result['error'] = str(e)\n    \n    return result\n\ndef main():\n    # Check command line arguments\n    if len(sys.argv) > 1:\n        mode = sys.argv[1].lower()\n        \n        if mode == \"export\":\n            # Export workspace to JSONL\n            output_file = sys.argv[2] if len(sys.argv) > 2 else \"workspace_export.jsonl\"\n            if not output_file.endswith('.jsonl'):\n                output_file += '.jsonl'\n            \n            print(\"=\"*80)\n            print(\"WORKSPACE EXPORT MODE\")\n            print(\"Exporting all files in workspace to JSONL format\")\n            print(\"=\"*80)\n            \n            try:\n                summary = export_workspace(output_file)\n                print(f\"\\nWorkspace export completed successfully!\")\n                return 0\n            except Exception as e:\n                print(f\"Error during export: {str(e)}\")\n                return 1\n        \n        elif mode == \"help\" or mode == \"-h\" or mode == \"--help\":\n            print(\"TrajectoryScraper - Smart Stopping Chain of Thought Version\")\n            print(\"=\"*70)\n            print(\"Usage:\")\n            print(\"  python TrajectoryScraper_final.py              # Run trajectory segmentation\")\n            print(\"  python TrajectoryScraper_final.py export       # Export workspace to JSONL\")\n            print(\"  python TrajectoryScraper_final.py export [file] # Export to custom file\")\n            print(\"  python TrajectoryScraper_final.py help         # Show this help\")\n            print(\"\\nModes:\")\n            print(\"  (default)  - Process trajectory files with smart stopping CoT segmentation\")\n            print(\"  export     - Export all workspace files to JSONL format\")\n            print(\"\\nSmart Stopping Features:\")\n            print(\"  - Looks for natural stopping points after 5000 tokens\")\n            print(\"  - Maximum segment size: 6144 tokens\")\n            print(\"  - All output consolidated to single JSONL file\")\n            print(\"  - Filters out boilerplate and back-and-forth conversation\")\n            print(\"  - Target preservation: 70% (focuses on substantial content)\")\n            print(\"  - Role normalization: Converts to system/user/assistant format\")\n            print(\"  - Supports both JSON and JSONL input formats\")\n            print(\"  - Optimized for high-quality chain of thought training\")\n            return 0\n    \n    # Default mode: trajectory segmentation with single output file\n    print(\"=\"*80)\n    print(\"SMART STOPPING CHAIN OF THOUGHT TRAJECTORY SEGMENTATION\")\n    print(\"Looks for natural stopping points after 5000 tokens, max 6144\")\n    print(\"All output consolidated to single JSONL file\")\n    print(\"Filters boilerplate and focuses on substantial reasoning content\")\n    print(\"Target preservation: 70% (selects best content for CoT learning)\")\n    print(\"Role normalization: Converts all roles to system/user/assistant format\")\n    print(\"=\"*80)\n    \n    # Find JSON files\n    json_files = []\n    for pattern in ['*.json', '*.jsonl']:\n        json_files.extend(Path('.').rglob(pattern))\n    \n    # Filter out already processed files\n    json_files = [f for f in json_files if not any(\n        s in f.name for s in ['_segmented', '_compressed', '_reasoning', '_highpres', '_final', 'comprehensive_export']\n    )]\n    \n    print(f\"Found {len(json_files)} JSON files to process.\")\n    \n    # Single output file for all segments\n    output_file = Path(\"consolidated_trajectories.jsonl\")\n    \n    results = []\n    total_segments_written = 0\n    \n    # Open single output file for all segments\n    with open(output_file, 'w', encoding='utf-8') as out_f:\n        for i, file_path in enumerate(json_files, 1):\n            print(f\"\\n[{i}/{len(json_files)}] Processing: {file_path.name}\")\n            \n            result = process_trajectory_file_to_single_file(file_path, out_f)\n            results.append(result)\n            \n            if result['success']:\n                total_segments_written += result['segments']\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"PROCESSING SUMMARY\")\n    print(\"=\"*80)\n    \n    successful = [r for r in results if r['success']]\n    failed = [r for r in results if not r['success']]\n    \n    if successful:\n        print(f\"Successfully processed: {len(successful)}/{len(results)} files\")\n        \n        total_segments = sum(r['segments'] for r in successful)\n        avg_segments = np.mean([r['segments'] for r in successful])\n        avg_preservation = np.mean([r['preservation'] for r in successful])\n        \n        print(f\"\\nTotal segments created: {total_segments}\")\n        print(f\"Average segments per file: {avg_segments:.1f}\")\n        print(f\"Average preservation: {avg_preservation:.1f}%\")\n        \n        # Verify preservation target (70% for focused CoT learning)\n        if 65.0 <= avg_preservation <= 75.0:\n            print(f\"OK Preservation rate WITHIN target range (65-75% for focused CoT)\")\n        else:\n            print(f\"! Preservation rate: {avg_preservation:.1f}% (target: 65-75% for focused CoT)\")\n        \n        # Show file details\n        print(f\"\\nFile details:\")\n        for result in successful:\n            print(f\"  {result['file']}:\")\n            print(f\"    - Segments: {result['segments']}\")\n            print(f\"    - Preservation: {result['preservation']:.1f}%\")\n            print(f\"    - Format: {result.get('file_format', 'Unknown')}\")\n    \n    if failed:\n        print(f\"\\nFailed files ({len(failed)}):\")\n        for result in failed:\n            print(f\"  {result['file']}: {result['error']}\")\n    \n    print(f\"\\nConsolidated output file: {output_file}\")\n    print(f\"Total segments written: {total_segments_written}\")\n    print(f\"Role normalization: Applied to all segments\")\n    print(f\"Smart stopping: Active (natural breaks after 5000 tokens, max 6144)\")\n\nif __name__ == \"__main__\":\n    exit(main())","content_preview":"import json\nimport tiktoken\nimport numpy as np\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport math\nimport mimetypes\nfrom datetime import datetime\nimport chardet\n\n# Initi","last_modified":"2025-12-14T18:53:04.387769","created_timestamp":"2025-12-14T17:53:22.645740","access_timestamp":"2025-12-14T18:54:31.611253"}
